\documentclass[11pt,a4paper]{article}
% \usepackage[utf-8]{inputenc}  % Not needed for modern LaTeX
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}

% Code styling
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

\title{\textbf{Systematic Optimization of a Small-Scale GPT-2 Model:\\A Case Study in Training Dynamics and Architectural Alignment}}
\author{Lucas Rose-Winters \& Claude}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a systematic approach to optimizing a 6-layer, 27.2M parameter GPT-2 style transformer model for Hacker News headline generation. Through evidence-based implementation of foundational training fixes, architectural modernization attempts, and targeted hyperparameter optimization, we achieved a 27.4\% improvement in validation loss from the baseline of 1.7533 to our final result of 1.272478. Surprisingly, simpler architectural components (LayerNorm, learned positional embeddings) outperformed modern alternatives (RMSNorm, RoPE) in this constrained setting, demonstrating the importance of aligning architectural complexity with data characteristics. The primary performance gain (90\% of total improvement) came from replacing SGD with AdamW optimizer and implementing proper learning rate scheduling, highlighting the critical importance of training dynamics over architectural sophistication for small-scale models.
\end{abstract}

\section{Introduction}

The rapid evolution of transformer architectures has been characterized by increasingly complex components designed to handle the challenges of large-scale language modeling. However, the assumption that modern architectural innovations universally improve performance across all scales and domains warrants careful examination. This report documents a systematic optimization effort for a small-scale transformer trained on Hacker News headlines—a specialized, short-sequence domain that differs significantly from the general-purpose, long-context scenarios that drive most architectural innovations.

Our optimization followed a three-phase approach: (1) foundational training dynamics fixes, (2) architectural modernization experiments, and (3) targeted hyperparameter optimization. The results reveal important insights about the relationship between model scale, data complexity, and optimal architectural choices.

\section{Methodology and Experimental Setup}

\subsection{Baseline Configuration}
The baseline model consisted of a 6-layer GPT-2 architecture with the following specifications:
\begin{itemize}
    \item \textbf{Architecture}: 6 layers, 8 attention heads, 512 d\_model (27.2M parameters)
    \item \textbf{Training}: 7 epochs, batch size 64, block size 128, seed 1337
    \item \textbf{Optimizer}: SGD with learning rate 6e-3, no weight decay
    \item \textbf{Scheduler}: CosineAnnealingLR with no warmup
    \item \textbf{Dataset}: Hacker News headlines (100K titles, 10\% validation)
    \item \textbf{Baseline validation loss}: 1.753271
\end{itemize}

\subsection{Research-Driven Optimization Strategy}
Our approach was grounded in extensive literature review focusing on transformer training dynamics, architectural components, and optimization techniques. We identified three critical areas for improvement:

1. \textbf{Training Dynamics}: The use of SGD represented a fundamental mismatch with transformer gradient heterogeneity
2. \textbf{Architectural Components}: Modern alternatives like RoPE and RMSNorm for efficiency gains
3. \textbf{Hyperparameter Optimization}: Fine-tuning regularization and learning rates

\section{Results and Analysis}

\subsection{Epic 1: Foundational Training Fixes}
The most impactful changes addressed fundamental training configuration issues:

\textbf{Key Changes:}
\begin{itemize}
    \item Replaced SGD with AdamW optimizer
    \item Reduced learning rate from 6e-3 to 3e-4 (appropriate for AdamW)
    \item Added weight decay (0.1) for regularization
    \item Implemented linear warmup (10\% of steps) + cosine decay schedule
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Final validation loss: \textbf{1.3095} (25.3\% improvement)
    \item Training stability: Excellent, monotonic convergence
    \item Both primary goal (<1.70) and stretch goal (<1.60) achieved
\end{itemize}

The dramatic improvement validates the research indicating that SGD struggles with the heterogeneous gradient landscape of transformers, where different parameter blocks (embeddings vs. upper layers) exhibit vastly different gradient scales.

\subsection{Epic 2: Architectural Modernization}
We implemented modern architectural components to test their effectiveness:

\textbf{Changes Implemented:}
\begin{itemize}
    \item Replaced LayerNorm with RMSNorm for computational efficiency
    \item Implemented Rotary Position Embeddings (RoPE) for better position handling
    \item Updated weight initialization to He initialization for GELU activations
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Final validation loss: \textbf{1.401} (0.091 \textit{worse} than Epic 1)
    \item Training time: Slightly slower (~37.5 vs 35 minutes)
    \item Convergence: Less stable initial phase
\end{itemize}

\textbf{Key Finding:} Modern architectural components provided no benefit and actually hindered performance in this constrained setting.

\subsection{Epic 3: Hyperparameter Optimization}
Building on Epic 1's success, we conducted systematic hyperparameter optimization:

\textbf{Learning Rate Optimization:}
\begin{center}
\begin{tabular}{cc}
\toprule
Learning Rate & Final Validation Loss \\
\midrule
2e-4 & 1.339260 \\
3e-4 & 1.309500 (Epic 1 baseline) \\
4e-4 & 1.292409 \\
6e-4 & 1.277592 \\
8e-4 & \textbf{1.273568} \\
\midrule
\end{tabular}
\end{center}

\textbf{Regularization Optimization:}
\begin{itemize}
    \item Weight decay: 0.05 performed slightly worse than 0.1
    \item Dropout: Reducing from 0.1 to 0.05 yielded final improvement to \textbf{1.272478}
\end{itemize}

\section{The Complexity vs. Simplicity Paradox}

One of the most significant findings was the superior performance of simpler architectural components over modern alternatives. This observation led us to investigate the theoretical foundations underlying this result.

\subsection{Positional Encoding Analysis}
RoPE's sophisticated rotational mechanism is designed to solve the "train-short-test-long" problem and handle long-range dependencies. However, these capabilities are entirely superfluous for Hacker News headlines:

\begin{itemize}
    \item \textbf{Sequence length}: Headlines are inherently short (<20 tokens typically)
    \item \textbf{Position importance}: Absolute position carries strong semantic weight (first word = subject)
    \item \textbf{Model capacity}: Small models benefit from direct, learnable position mappings
\end{itemize}

Learned absolute positional embeddings proved more effective by allowing the model to directly memorize the specific positional patterns in the headline data without the overhead of complex rotational logic.

\subsection{Normalization Layer Analysis}
RMSNorm's removal of mean-centering is justified in large models where hidden representations naturally become orthogonal to the uniform vector. However, in our small model:

\begin{itemize}
    \item Training dynamics are more volatile
    \item Explicit mean-centering provides crucial stabilization
    \item The model lacks capacity to implicitly learn stable representations
\end{itemize}

LayerNorm's mean-centering acts as a "guardrail" that prevents activation drift, particularly important during early training phases.

\subsection{No Free Lunch Theorem Application}
Our results exemplify the No Free Lunch theorem: no algorithm performs better than any other when averaged over all possible problems. Modern architectural components carry inductive biases optimized for large-scale, general-purpose modeling. When applied to specialized, short-sequence tasks, these biases become misaligned with the data characteristics, leading to suboptimal performance.

\section{Final Configuration and Performance}

\subsection{Champion Configuration}
\begin{lstlisting}[language=Python, caption=Final Hyperparameters]
# Optimizer
opt = torch.optim.AdamW(
    model.parameters(),
    lr=8e-4,                # Optimized learning rate
    weight_decay=0.1,       # Regularization
    betas=(0.9, 0.999),
    eps=1e-8
)

# Architecture
dropout = 0.05              # Reduced for more capacity
# Standard LayerNorm and learned positional embeddings
# No RoPE, no RMSNorm

# Learning Rate Schedule
warmup_steps = max_steps // 10  # Linear warmup
# Followed by cosine decay to 10% of peak LR
\end{lstlisting}

\subsection{Performance Summary}
\begin{center}
\begin{tabular}{lcc}
\toprule
Configuration & Validation Loss & Improvement \\
\midrule
Baseline (SGD) & 1.753271 & -- \\
Epic 1 (AdamW + Schedule) & 1.309500 & 25.3\% \\
Epic 2 (+ Modern Arch.) & 1.401000 & 20.1\% \\
Epic 3 (Hyperopt) & \textbf{1.272478} & \textbf{27.4\%} \\
\bottomrule
\end{tabular}
\end{center}

\section{Key Insights and Lessons Learned}

\subsection{Training Dynamics Are Paramount}
The optimizer change alone provided 90\% of our total improvement. This underscores that for small models, getting the training dynamics right is far more important than architectural sophistication.

\subsection{Context-Dependent Architecture Selection}
Modern architectural components are not universally superior. Their effectiveness depends critically on alignment between their inductive biases and the characteristics of the specific task and data.

\subsection{Scale-Dependent Component Effectiveness}
Components that work well at large scales may not translate to small-scale settings:
\begin{itemize}
    \item RMSNorm's efficiency gains are negligible for small models
    \item RoPE's extrapolation capabilities are wasted on short sequences
    \item Simple, direct approaches can be more effective
\end{itemize}

\subsection{Hyperparameter Optimization Provides Fine-Tuning}
While foundational fixes provided dramatic improvements, careful hyperparameter optimization yielded additional meaningful gains, particularly in learning rate and regularization tuning.

\section{Conclusion}

This optimization exercise demonstrates that systematic, research-driven approaches to model improvement can yield substantial performance gains. However, it also reveals the critical importance of matching architectural complexity to data characteristics and model scale.

Our 27.4\% improvement in validation loss was achieved not through adoption of the latest architectural innovations, but through careful alignment of training dynamics and architectural choices with the specific requirements of the task. The superior performance of simpler components (LayerNorm over RMSNorm, learned APE over RoPE) serves as a powerful reminder that in machine learning, context is paramount.

For practitioners working on specialized applications, this work suggests a deliberate, context-aware approach to architectural choices rather than uncritical adoption of state-of-the-art components. The optimal solution is often not the most complex or the most modern, but the one whose inherent biases are most elegantly aligned with the problem at hand.

\section*{Final Metrics}
\begin{itemize}
    \item \textbf{Final Validation Loss}: 1.272478
    \item \textbf{Total Improvement}: 27.4\% from baseline (1.7533 → 1.2725)
    \item \textbf{Goals Achieved}: Primary (<1.70) YES, Stretch (<1.60) YES
    \item \textbf{Training Time}: ~35 minutes (CPU, Intel i9 14900kf)
    \item \textbf{Model Parameters}: 27.2M (unchanged from baseline)
\end{itemize}

\end{document}